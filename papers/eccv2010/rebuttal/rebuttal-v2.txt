A key concern raised by all three reviewers is the apparently
restrictive nature of the proposed indoor Manhattan models. It must
therefore be stressed that our aim is to recover _coarse_,
_high-level_ models in order to recover useful geometric cues that
will assist further reasoning, rather than to recover intricately
detailed 3D models of the environment.  The indoor Manhattan model is
useful in our context because (i) it represents a very good trade-off
between capturing key geometric properties of a scene (such as the
dominant surface orientations, height of the floor and ceiling w.r.t
the camera, and the position of major room boundaries) while retaining
a low-dimensional parameterization; (ii) the positions of room
boundaries, which our model recovers explicitly, are more salient for
purposes such as navigation, scene classification, or contextual
priming of object detection, than the detailed geometry of room
clutter.  We will aim to make these points even clearer in the final
version.

We note that the choice of indoor Manhattan models constitutes an
effort to recover room boundaries _in spite_ of scene clutter, and
indeed our system is able to achieve precisely this (see rows 1-7 of
Fig 2 in supplementary material).  These figures we hope explicitly
answer the criticisms of XXX and XXX who questioned the robustness to
clutter.

We further note that previous attempts at single-view reconstruction
have (necesarily) encountered the same robustness/complexity
trade-off, and most have indeed chosen to restrict the class of scene
models. Lee [4] and Flint [5] employ exactly the same class of models
as us, while Hedau et al. [12] choose the far more restricted class of
simple cuboids. Hoiem et al. [10] also restricts surface orientations
to three dominant directions; only Saxena et al. [11] allow more
general models, though they do not evaluate their system on difficult
indoor scenes.

R2 requests that we include the optimal substructure proof in
the main text. We agree and have re-worked the layout of the final
paper in order to achieve this.

We regret overlooking the work of Barinova et al, and this will be
rectified in the final version.  Their work does not overlap with our
approach (they use CRFs together with an EM algorithm) nor is a
comparison with their results appropriate (their approach is suited
only to outdoor scenes).  The work of Felzenszwalb noted by R3 is to
appear in CVPR2010.  Kt was therefore not avaialble at the time of our
submission.  We have subsequently noted that their approach is quite
different to ours.

R2 questions the legitimacy of our evaluation framework.  While there
are indeed issues with the choice of the pixelwise evaluation metric,
this is what others have previously used (Hoiem [10], Lee [4], and
Hedau [12]), and so we also adopted it to allow direct comparisons
with those approaches.

R5 construes our approach as a "smoother" of the provided orientation
estimate.  To clarify the relationship of our work to Lee et al: we
employ their plane sweep heuristic to obtain noisy orientation
estimates as depicted in the second column of figures 3, 4, 5, and 6
(supplementary material), but our system does _not_ use the output of
the branch-and-bound algorithm they employ; instead, our dynamic
programming solution represents a much more efficient alternative to
their algorithm and we _compare_ our results to theirs.

Finally, R5 questions the method of ground truth acquisition: it was
obtained by hand-labelling structure-from-motion maps as described in
the first paragraph of section 7. Small inaccuracies may arise from
human error or structure-from-motion errors, but we do not believe
there is any systematic bias.
