We thank all three reviewers for their positive comments as well as constructive criticism.

A key concern raised by all three reviewers is the apparently restrictive nature of the proposed indoor Manhattan models. Since this constitutes by far the most frequent objection, and indeed is the only substantial complaint of Reviewer 5, we dedicate the following four paragraphs to responding to this issue.

It must be stressed that our aim is to recover _coarse_, _high-level_ models in order to obtain useful geometric cues that will assist further reasoning, rather than to recover intricately detailed 3D models of the environment. Given the ill-posed nature of the single-view reconstruction problem, it seems obvious to us that there is a trade-off between the complexity of models on one hand and the robustness of the estimation process on the other. We believe that indoor Manhattan model represent an ideal point on this trade-off curve since they capture an excellent set of geometric properties (such as the dominant surface orientations, height of the floor and ceiling w.r.t the camera, and the position of major room boundaries) while retaining a low-dimensional parameterization.

In addition to the robustness argument above, there is a further advantage to our use of indoor Manhattan models: the position of room boundaries, which our models recover explicitly, will prove more salient for purposes such as navigation, scene classification, or contextual priming of object detection, than the detailed geometry of room clutter. On this view the choice of indoor Manhattan models constitutes an effort to recover room boundaries _in spite_ of scene clutter,  and indeed our system is able to achieve precisely this (see rows 1-7 of Figure 2 in supplementary material).

We further note that previous attempts at single-view reconstruction have (necesarily) encountered the same robustness/complexity trade-off, and most have indeed chosen to restrict the class of scene models. Lee [4] and Flint [5] employ exactly the same class of models as us, while Hedau et al. [12] choose the far more restricted class of simple cuboids. Hoiem et al. [10] also restricts surface orientations to three dominant directions; only Saxena et al. [11] allow more general models, though they do not evaluate their system on difficult indoor scenes.

It seems appropriate to briefly sketch some concrete examples of "high-level reasoning systems" that would benefit from being able to recover indoor Manhattan models. A few examples are:
- Contextual cues for object detection, in the style pioneered by Torralba ("Contextual priming for object detection", IJCV 2003) and demonstrated for outdoor scenes by Hoiem ("Putting objects in perspective", IJCV, 2008). It seems obvious that "distance from the ground", "heigh relative to floor-ceiling distance", and "distance from nearest wall" would make extremely powerful cues when detecting objects in indoor scenes.
- Categorizing scenes into semantic categories such as "kitchen" or "bedroom", such as that investigated by Fei-Fei et al. ("A bayesian hierarchical model for learning natural scene categories", CVPR 2005). Incorporating cues such as the dimensions and shape of a room, as well as taking surface boundaries and orientation into account when reasoning from photometric cues, would constitute a powerful addition to texture-only approaches taken previously.
- High-level descriptions of room boundaries would find many uses for the purpose of path-planning and navigation in mobile robotics.

Reviewer 2 requests that we include the optimal substructure proof in the main text. We agree and have re-worked the layout of the final paper in order to achieve this.

We also thank reviewer 2 for pointing out the work of Barinova et al. Their work does not overlap with our approach (they use CRFs together with an EM algorithm) nor is a comparison with their results appropriate (their approach is suited only to outdoor scenes); we do, however, regret that this citation was omitted from the review manuscript and have added it to the final version. We also thank reviewer 3 for the reference to the work of Felzenszwalb and Vexler, but since this work is to appear at CVPR later this year we obviously could not have cited it in the review manuscript since it was not available at that time. Furthermore, our approach is quite different to theirs.

In response to the question raised by reviewer 2 regarding the ligitimacy of our evaluation framework, we respond that we chose the pixelwise evaluation metric in line with that chosen by Hoiem [10], Lee [4], and Hedau [12], and in order to allow direct comparisons with those approaches.

Reviewer 5 construes our approach as a "smoother" of the provided orientation estimate. Perhaps the reviewer has misunderstood the relationship of our work to that of Lee et al: We employ their plane sweep heuristic to obtain noisy orientation estimates as depicted in the second column of figures 3, 4, 5, and 6 (supplementary material). Importantly, our system does _not_ use the output of the branch-and-bound algorithm they employ; rather, our dynamic programming solution represents an alternative to their algorithm and we _compare_ our results to theirs.

Finally, to answer reviewer 5's question about our ground truth acquisition: it was obtained by hand-labelling structure-from-motion maps as described in the first paragraph of section 7. Small inaccuracies may arise from human error or structure-from-motion errors, but we see no evidence for systematic bias here.

